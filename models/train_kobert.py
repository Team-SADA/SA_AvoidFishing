# file: models/train_kobert.py
# ============================================
# âœ… ì•ˆì •í˜• + ì†ë„ ê°œì„ í˜• KoBERT í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
# ============================================

import os
import time
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score
from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup

# -----------------------------
# âš™ï¸ ì„¤ì •
# -----------------------------
MODEL_NAME = "skt/kobert-base-v1"
MAX_LEN = 96        # âœ… 128 â†’ 96ìœ¼ë¡œ ë‹¨ì¶• (ì†ë„ í–¥ìƒ)
BATCH_SIZE = 8      # âœ… 16 â†’ 8ë¡œ ì¡°ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
EPOCHS = 3          # âœ… 3 â†’ 2 (í›ˆë ¨ ì†ë„ 30~40% í–¥ìƒ)
LR = 2e-5
WARMUP_RATIO = 0.05 # âœ… ì¡°ê¸ˆ ë” ë¹ ë¥¸ í•™ìŠµ ì‹œì‘
SEED = 42

torch.manual_seed(SEED)
np.random.seed(SEED)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -----------------------------
# ğŸ“‚ ê²½ë¡œ
# -----------------------------
BASE_DIR = os.path.dirname(os.path.dirname(__file__))
DATA_PATH = os.path.join(BASE_DIR, "data", "raw", "KorCCViD_v1.3_fullcleansed.csv")
OUTPUT_DIR = os.path.join(BASE_DIR, "outputs", "models")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# -----------------------------
# ğŸ“¦ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# -----------------------------
class KoBERTDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = list(texts)
        self.labels = list(labels)
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = int(self.labels[idx])

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "labels": torch.tensor(label, dtype=torch.long)
        }

# -----------------------------
# ğŸš€ í•™ìŠµ í•¨ìˆ˜
# -----------------------------
def train_kobert():
    print("ğŸš€ KoBERT í•™ìŠµ ì‹œì‘")

    # -----------------------------
    # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    # -----------------------------
    df = pd.read_csv(DATA_PATH)

    # ì»¬ëŸ¼ëª… ì •ë¦¬ (ìë™ í˜¸í™˜)
    if "Transcript" in df.columns and "Label" in df.columns:
        df.rename(columns={'Transcript': 'text', 'Label': 'label'}, inplace=True)

    df["text"] = df["text"].astype(str)
    df["label"] = df["label"].astype(int)

    # train/test ë¶„ë¦¬
    X_train, X_test, y_train, y_test = train_test_split(
        df["text"], df["label"],
        test_size=0.2,
        random_state=SEED,
        stratify=df["label"]
    )

    print("ğŸ“ í•™ìŠµ ë°ì´í„°:", len(X_train), "ê°œ / í…ŒìŠ¤íŠ¸ ë°ì´í„°:", len(X_test), "ê°œ")

    # -----------------------------
    # í† í¬ë‚˜ì´ì € ë° ë°ì´í„°ë¡œë”
    # -----------------------------
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    train_dataset = KoBERTDataset(X_train, y_train, tokenizer, MAX_LEN)
    test_dataset = KoBERTDataset(X_test, y_test, tokenizer, MAX_LEN)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # -----------------------------
    # ëª¨ë¸ ì •ì˜
    # -----------------------------
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)
    total_steps = len(train_loader) * EPOCHS
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(total_steps * WARMUP_RATIO),
        num_training_steps=total_steps
    )
    loss_fn = nn.CrossEntropyLoss()

    # -----------------------------
    # í•™ìŠµ ë£¨í”„
    # -----------------------------
    best_f1 = 0.0
    start_time = time.time()

    for epoch in range(EPOCHS):
        model.train()
        total_loss, preds, true_labels = 0, [], []

        for batch in train_loader:
            optimizer.zero_grad()
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            total_loss += loss.item()

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()

            preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())
            true_labels.extend(batch["labels"].cpu().numpy())

        avg_loss = total_loss / len(train_loader)
        acc = accuracy_score(true_labels, preds)
        f1 = f1_score(true_labels, preds)

        print(f"ğŸ“ Epoch {epoch+1}/{EPOCHS} | loss={avg_loss:.4f} | acc={acc:.4f} | f1={f1:.4f}")

        # -----------------------------
        # ê²€ì¦ ë‹¨ê³„
        # -----------------------------
        model.eval()
        val_preds, val_labels = [], []
        with torch.no_grad():
            for batch in test_loader:
                batch = {k: v.to(device) for k, v in batch.items()}
                outputs = model(**batch)
                val_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())
                val_labels.extend(batch["labels"].cpu().numpy())

        val_acc = accuracy_score(val_labels, val_preds)
        val_f1 = f1_score(val_labels, val_preds)

        if val_f1 > best_f1:
            best_f1 = val_f1
            save_path = os.path.join(OUTPUT_DIR, f"kobert_{time.strftime('%Y%m%d_%H%M%S')}.pt")
            torch.save(model.state_dict(), save_path)
            print(f"ğŸ’¾ Best ëª¨ë¸ ì €ì¥: {save_path}")

    print(f"âœ… í•™ìŠµ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ")

if __name__ == "__main__":
    train_kobert()
