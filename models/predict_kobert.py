# file: models/predict_kobert.py
# ============================================
# âœ… KoBERT ì‹¤ì‹œê°„ ë¬¸ì¥ ì˜ˆì¸¡ (ì•ˆì • ë²„ì „, ì™¸ë¶€ ì˜ì¡´ì„± ì œê±°)
# ============================================

import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# -----------------------------
# âš™ï¸ ì„¤ì •
# -----------------------------
MODEL_NAME = "skt/kobert-base-v1"
MAX_LEN = 96
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -----------------------------
# ğŸ“‚ ê²½ë¡œ
# -----------------------------
BASE_DIR = os.path.dirname(os.path.dirname(__file__))
MODEL_DIR = os.path.join(BASE_DIR, "outputs", "models")

# -----------------------------
# ğŸ“¦ ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
# -----------------------------
def load_latest_kobert():
    """
    outputs/models/ í´ë”ì˜ ìµœì‹  KoBERT ëª¨ë¸(.pt) ë¡œë“œ
    """
    model_files = sorted(
        [f for f in os.listdir(MODEL_DIR) if f.startswith("kobert") and f.endswith(".pt")],
        reverse=True
    )
    if not model_files:
        raise FileNotFoundError("âŒ í•™ìŠµëœ KoBERT ëª¨ë¸(.pt)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    model_path = os.path.join(MODEL_DIR, model_files[0])
    print(f"âœ… ìµœì‹  KoBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")

    # âœ… AutoTokenizer ì‚¬ìš© (kobert_tokenizer ëŒ€ì²´)
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        use_fast=False,  # KoBERTëŠ” slow tokenizerê°€ ë” ì•ˆì •ì 
        do_lower_case=False
    )

    # âœ… ëª¨ë¸ ë¡œë“œ
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()

    return tokenizer, model

# -----------------------------
# ğŸ” ì˜ˆì¸¡ í•¨ìˆ˜
# -----------------------------
def predict_text_kobert(text, tokenizer, model):
    """
    KoBERTë¥¼ ì‚¬ìš©í•œ ë¬¸ì¥ ì˜ˆì¸¡ (ì •ìƒ / ë³´ì´ìŠ¤í”¼ì‹±)
    """
    inputs = tokenizer(
        text,
        truncation=True,
        padding='max_length',
        max_length=MAX_LEN,
        return_tensors='pt'
    ).to(device)

    # ğŸš« token_type_ids ì œê±° (KoBERTëŠ” ë‹¨ë¬¸ ê¸°ì¤€)
    if "token_type_ids" in inputs:
        inputs.pop("token_type_ids")

    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)[0]
        pred = torch.argmax(probs).item()
        confidence = probs[pred].item() * 100

    label = "âš ï¸ ë³´ì´ìŠ¤í”¼ì‹± ì˜ì‹¬" if pred == 1 else "âœ… ì •ìƒ í†µí™”"
    print(f"\nğŸ’¬ ì…ë ¥ ë¬¸ì¥: {text}")
    print(f"ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼: {label} ({confidence:.2f}% í™•ë¥ )")

    if pred == 1 and confidence >= 50:
        print("ğŸš¨ KoBERT ëª¨ë¸ì—ì„œ ë†’ì€ ìœ„í—˜ë„ë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤.\n")

# -----------------------------
# ğŸ§  ì‹¤í–‰
# -----------------------------
if __name__ == "__main__":
    tokenizer, model = load_latest_kobert()

    while True:
        text = input("ğŸ“ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit): ").strip()
        if text.lower() == "exit":
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        predict_text_kobert(text, tokenizer, model)
