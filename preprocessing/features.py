# file: preprocessing/features.py

import re
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import AutoTokenizer, AutoModel
import torch

# ------------------------------------------------------
#  ìˆ«ì ë° í‚¤ì›Œë“œ ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜
# ------------------------------------------------------
def extract_text_features(texts: list[str]) -> pd.DataFrame:
    """
    í…ìŠ¤íŠ¸ ê¸°ë°˜ í†µê³„ì  íŠ¹ì§•(feature) ì¶”ì¶œ
    """
    money_keywords = ["ê³„ì¢Œ", "ì†¡ê¸ˆ", "ì…ê¸ˆ", "ì¶œê¸ˆ", "ëŒ€ì¶œ", "ì´ì²´", "ì¹´ë“œ", "í†µì¥", "ì€í–‰", "ë³´ë‚´"]
    action_keywords = ["í™•ì¸", "ì…ë ¥", "ì „ë‹¬", "ì „í™”", "ì—°ë½", "ì‘ë‹µ", "í´ë¦­", "ë“±ë¡", "ì…ë ¥í•˜ì„¸ìš”"]

    features = {
        "text_length": [],
        "num_words": [],
        "num_count": [],
        "money_keyword": [],
        "action_keyword": []
    }

    for text in texts:
        text = str(text)
        features["text_length"].append(len(text))
        features["num_words"].append(len(text.split()))
        features["num_count"].append(len(re.findall(r"\d+", text)))

        features["money_keyword"].append(sum(kw in text for kw in money_keywords))
        features["action_keyword"].append(sum(kw in text for kw in action_keywords))

    return pd.DataFrame(features)


# ------------------------------------------------------
#  TF-IDF ë²¡í„°í™” í•¨ìˆ˜
# ------------------------------------------------------
def get_tfidf_features(texts: list[str], max_features: int = 3000):
    vectorizer = TfidfVectorizer(max_features=max_features)
    tfidf_matrix = vectorizer.fit_transform(texts)
    print(f" TF-IDF í”¼ì²˜ ìƒì„± ì™„ë£Œ ({tfidf_matrix.shape[1]}ì°¨ì›)")
    return tfidf_matrix, vectorizer


# ------------------------------------------------------
#  KoBERT ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜
# ------------------------------------------------------
def get_kobert_embeddings(texts: list[str], model_name="skt/kobert-base-v1", max_len=64):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name).to(device)

    embeddings = []
    for text in texts:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=max_len).to(device)
        with torch.no_grad():
            outputs = model(**inputs)
        cls_vector = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # [CLS] í† í° ë²¡í„°
        embeddings.append(cls_vector[0])

    embeddings = np.array(embeddings)
    print(f" KoBERT ì„ë² ë”© ìƒì„± ì™„ë£Œ ({embeddings.shape})")
    return embeddings


# ------------------------------------------------------
#  ì „ì²´ í”¼ì²˜ ê²°í•© í•¨ìˆ˜
# ------------------------------------------------------
def build_features(texts: list[str], use_kobert=True):
    """
    ëª¨ë“  í”¼ì²˜ë¥¼ ê²°í•©í•˜ì—¬ X(feature matrix) ìƒì„±
    """
    # í…ìŠ¤íŠ¸ ê¸°ë°˜ í†µê³„ í”¼ì²˜
    df_features = extract_text_features(texts)

    # TF-IDF í”¼ì²˜
    tfidf_matrix, _ = get_tfidf_features(texts)

    # KoBERT ì„ë² ë”© (ì„ íƒ)
    if use_kobert:
        bert_features = get_kobert_embeddings(texts)
        X = np.concatenate([tfidf_matrix.toarray(), bert_features, df_features.values], axis=1)
    else:
        X = np.concatenate([tfidf_matrix.toarray(), df_features.values], axis=1)

    print(f" ì „ì²´ í”¼ì²˜ ê²°í•© ì™„ë£Œ: shape={X.shape}")
    return X, df_features.columns.tolist()


# ------------------------------------------------------
# 5ï¸âƒ£ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
# ------------------------------------------------------
if __name__ == "__main__":
    sample_texts = [
        "ì•ˆë…•í•˜ì„¸ìš”, ê²€ì°°ì²­ì…ë‹ˆë‹¤. ê·€í•˜ì˜ ê³„ì¢Œê°€ ë²”ì£„ì— ì—°ë£¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì°¸ ì¢‹ë„¤ìš”!",
        "ëŒ€ì¶œ ìŠ¹ì¸ì„ ìœ„í•´ ì¸ì¦ë²ˆí˜¸ 1234ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    ]

    X, feature_names = build_features(sample_texts, use_kobert=False)
    print("âœ… Feature matrix shape:", X.shape)
    print("ğŸ“Š Features:", feature_names)
