# file: preprocessing/tokenize.py

import re
from typing import List
from transformers import AutoTokenizer
import nltk

# NLTK ë‹¤ìš´ë¡œë“œ (ì²˜ìŒ ì‹¤í–‰ ì‹œ ìë™ ì„¤ì¹˜)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')


# ------------------------------------------------------
# 1ï¸âƒ£ KoBERTìš© AutoTokenizer
# ------------------------------------------------------
def get_kobert_tokenizer(model_name: str = "skt/kobert-base-v1"):
    """
    KoBERTìš© SentencePiece ê¸°ë°˜ AutoTokenizer ë¡œë“œ
    (protobuf ë° ìºì‹œ ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨)
    """
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print(f"âœ… KoBERT tokenizer ë¡œë“œ ì™„ë£Œ: {model_name}")
        return tokenizer

    except ImportError as e:
        print("âŒ Protobuf ê´€ë ¨ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        print("ğŸ”§ í•´ê²° ë°©ë²•:")
        print("  1ï¸âƒ£ pip uninstall protobuf -y")
        print("  2ï¸âƒ£ pip install protobuf==3.20.3")
        print("  3ï¸âƒ£ ê°€ìƒí™˜ê²½ ì¬ì‹œì‘ í›„ ë‹¤ì‹œ ì‹¤í–‰")
        raise e

    except Exception as e:
        print(f"âŒ KoBERT tokenizer ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise e


# ------------------------------------------------------
# 2ï¸âƒ£ KoBERT ì…ë ¥ ë³€í™˜ í•¨ìˆ˜
# ------------------------------------------------------
def tokenize_for_kobert(texts: List[str], tokenizer, max_len: int = 64):
    """
    KoBERT ëª¨ë¸ì— ë§ê²Œ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ì—¬
    PyTorch tensor í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if isinstance(texts, str):
        texts = [texts]

    encoded = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=max_len,
        return_tensors="pt"  # PyTorch tensorë¡œ ë°˜í™˜
    )

    return encoded


# ------------------------------------------------------
# 3ï¸âƒ£ TF-IDFìš© ê°„ë‹¨ í† í°í™” í•¨ìˆ˜
# ------------------------------------------------------
def tokenize_for_tfidf(texts: List[str]) -> List[str]:
    """
    TF-IDF ì…ë ¥ìš© ë‹¨ìˆœ ë‹¨ì–´ ê¸°ë°˜ í† í°í™”
    (í•œê¸€ê³¼ ìˆ«ìë§Œ ìœ ì§€, ë„ì–´ì“°ê¸° ê¸°ì¤€)
    """
    tokenized_texts = []
    for text in texts:
        cleaned = re.sub(r"[^ê°€-í£0-9\s]", " ", str(text))  # í•œê¸€/ìˆ«ì ì™¸ ì œê±°
        tokens = cleaned.split()  # ë„ì–´ì“°ê¸° ê¸°ì¤€ í† í°í™”
        tokens = [t for t in tokens if len(t) > 1]  # í•œ ê¸€ì ì œê±°
        tokenized_texts.append(" ".join(tokens))
    return tokenized_texts


# ------------------------------------------------------
# 4ï¸âƒ£ MeCab í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í† í°í™” (ì„ íƒì )
# ------------------------------------------------------
def tokenize_with_mecab(texts: List[str], mecab=None) -> List[str]:
    """
    í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í•œêµ­ì–´ í† í°í™” (ì„ íƒì )
    """
    if mecab is None:
        from konlpy.tag import Mecab
        mecab = Mecab()

    tokenized_texts = []
    for text in texts:
        tokens = mecab.morphs(text)
        tokenized_texts.append(" ".join(tokens))
    return tokenized_texts


# ------------------------------------------------------
# 5ï¸âƒ£ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ (ë‹¨ë… ì‹¤í–‰ ì‹œ)
# ------------------------------------------------------
if __name__ == "__main__":
    sample_texts = [
        "ì•ˆë…•í•˜ì„¸ìš”, ê²€ì°°ì²­ì…ë‹ˆë‹¤. ê·€í•˜ì˜ ê³„ì¢Œê°€ ë²”ì£„ì— ì—°ë£¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì°¸ ì¢‹ë„¤ìš”!",
        "ëŒ€ì¶œ ìŠ¹ì¸ì„ ìœ„í•´ ì¸ì¦ë²ˆí˜¸ 1234ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    ]

    # KoBERT í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
    try:
        tokenizer = get_kobert_tokenizer()
        encoded = tokenize_for_kobert(sample_texts, tokenizer)
        print("\nğŸ§  KoBERT Tokenized Example:")
        print(encoded["input_ids"][0][:10])  # ì¼ë¶€ë§Œ ì¶œë ¥
    except Exception as e:
        print("\nâš ï¸ KoBERT í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨:", e)

    # TF-IDF í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
    print("\nâš¡ TF-IDF Tokenized Example:")
    tfidf_tokens = tokenize_for_tfidf(sample_texts)
    for t in tfidf_tokens:
        print("-", t)

    # MeCab í…ŒìŠ¤íŠ¸ (ì„ íƒ)
    try:
        mecab_tokens = tokenize_with_mecab(sample_texts)
        print("\nğŸª„ MeCab Tokenized Example:")
        for t in mecab_tokens:
            print("-", t)
    except Exception:
        print("\nâš ï¸ MeCabì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í˜•íƒœì†Œ ë¶„ì„ ìŠ¤í‚µ.")
